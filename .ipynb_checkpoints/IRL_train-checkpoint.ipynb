{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import gym\n",
    "from env.StockTradingEnv2 import StockTradingEnv\n",
    "import pandas as pd\n",
    "from FinMind.data import DataLoader\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import csv\n",
    "import numpy as np\n",
    "from imitation.data import types\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import seals\n",
    "import os\n",
    "\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "api_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJkYXRlIjoiMjAyMS0xMi0yNyAxNDo1OTowOSIsInVzZXJfaWQiOiJkdXJhbnQ3MTA5MTYiLCJpcCI6IjE0MC4xMjAuMTMuMjMwIn0.8-KIC3-OA4D6JcOtQ_fJBOVkyugx60t1Gy82c57TLz4\"\n",
    "\n",
    "api = DataLoader()\n",
    "api.login_by_token(api_token = api_token)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定股票標的和開始/結束日期\n",
    "stock_list = ['2330', '2603', '2002','1301', '2801']\n",
    "stock_list = ['2330', '2603']\n",
    "strategy_list = ['SMA', 'KD', 'BBAND']\n",
    "#strategy_list = ['BBAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_list = []\n",
    "\n",
    "with open(\"./data_new/Trajectory/Train/2330_KD_trajectory_50_train_new.csv\", 'r', encoding='utf8', newline='') as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            for r in reader:\n",
    "                trajectory_list.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectory_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2330 SMA 20\n"
     ]
    }
   ],
   "source": [
    "for stock_id in stock_list:\n",
    "    for strategy in strategy_list:   \n",
    "        #stock_id = \"2330\"\n",
    "        #strategy = \"SMA\"\n",
    "        day_length = 20\n",
    "\n",
    "        start_date='2004-01-01'\n",
    "        end_date='2020-12-31'\n",
    "        #start_date='2013-01-01'\n",
    "        #start_date = '2021-01-01'\n",
    "        #end_date = '2021-12-31'\n",
    "\n",
    "\n",
    "        print(stock_id, strategy, day_length)\n",
    "\n",
    "        # 股價日成交資訊\n",
    "        df = api.taiwan_stock_daily(\n",
    "            stock_id = stock_id,\n",
    "            start_date = start_date,\n",
    "            end_date = end_date\n",
    "        )\n",
    "\n",
    "        #df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_' + str(day_length) + \"_nochip\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_30.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_20_old_price2.csv',header=None)\n",
    "        df2 = pd.read_csv('./data_new/Input3/' + stock_id + '_input_train_20_train.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/' + stock_id + '_input_train_' + str(day_length) + \"_NEW\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/' + stock_id + '_input_train_' + str(day_length) + \"_NEW\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/temp/' + stock_id + '_input_train_' + str(day_length) + '.csv',header=None)\n",
    "        \n",
    "        print(len(df))\n",
    "        #print(df2)\n",
    "        print(len(df2))\n",
    "\n",
    "        venv = DummyVecEnv([lambda: StockTradingEnv(df, df2)])\n",
    "\n",
    "        expert = PPO(\n",
    "            policy=MlpPolicy,\n",
    "            env=venv,\n",
    "            seed=0,\n",
    "            batch_size=64,\n",
    "            ent_coef=0.0,\n",
    "            learning_rate=0.0003,\n",
    "            n_epochs=10,\n",
    "            n_steps=64,\n",
    "        )\n",
    "\n",
    "        trajectory_list = []\n",
    "\n",
    "        filename = stock_id + '_' + strategy + '_trajectory_50_train_new.csv'\n",
    "        #filename = stock_id + '_' + strategy + '_trajectory_50_test.csv'\n",
    "\n",
    "        #filename = '2330_ZIGZAG_trajectory_0.02.csv'\n",
    "\n",
    "        #\"\"\"\n",
    "        with open(\"./data_new/Trajectory/Train/\" + filename, 'r', encoding='utf8', newline='') as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            for r in reader:\n",
    "                trajectory_list.append(r)\n",
    "        #\"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        with open(\"./data/Trajectory/Test/\" + filename, 'r', encoding='utf8', newline='') as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            for r in reader:\n",
    "                trajectory_list.append(r)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        final_trajectory = []\n",
    "        state = 0\n",
    "\n",
    "        # 0: all cash\n",
    "        # 1: all stock\n",
    "\n",
    "        for trajectory in trajectory_list:\n",
    "            if trajectory[1] == 'buy':\n",
    "                final_trajectory.append(1)\n",
    "                state = 1\n",
    "            elif trajectory[1] == 'sell':\n",
    "                final_trajectory.append(0)\n",
    "                state = 0\n",
    "            else:\n",
    "                if state == 0:\n",
    "                    final_trajectory.append(0)\n",
    "                else:\n",
    "                    final_trajectory.append(1)\n",
    "\n",
    "        shares_held = 0\n",
    "        reward_list = []\n",
    "\n",
    "        for i in range(len(final_trajectory)):\n",
    "            price = df.loc[i, \"close\"]\n",
    "            try:\n",
    "                previous_price = df.loc[i - 1, \"close\"]\n",
    "            except:\n",
    "                previous_price = df.loc[i, \"open\"]\n",
    "\n",
    "            if final_trajectory[i] == 0:\n",
    "                if shares_held > 0:\n",
    "                    sell_at_price = price\n",
    "                    shares_held = 0\n",
    "                    actual_action = 1\n",
    "                else:\n",
    "                    actual_action = 2\n",
    "            elif final_trajectory[i] == 1:\n",
    "                if shares_held == 0:\n",
    "                    buy_at_price = price\n",
    "                    shares_held = 1\n",
    "                    actual_action = 0\n",
    "                else:\n",
    "                    actual_action = 2\n",
    "\n",
    "            current_price = df.loc[i, \"close\"]\n",
    "\n",
    "\n",
    "            RR = (current_price - previous_price) / previous_price\n",
    "\n",
    "            if actual_action == 0:\n",
    "                # buy\n",
    "                reward = RR\n",
    "\n",
    "            elif actual_action == 1:\n",
    "                # sell\n",
    "                # 賣的時候就是看收益率/投資報酬率\n",
    "                RoR = (sell_at_price - buy_at_price) / buy_at_price\n",
    "                reward = RoR\n",
    "\n",
    "            elif actual_action == 2:\n",
    "                # hold\n",
    "                # RR正的時候盡量持有，負的時候就不要持有\n",
    "                reward = RR\n",
    "\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        t_action = np.array(final_trajectory)\n",
    "        t_reward = np.array(reward_list)\n",
    "\n",
    "        trajectory = types.TrajectoryWithRew(obs=df2, acts=t_action, infos=None, terminal=True, rews=t_reward)\n",
    "\n",
    "\n",
    "        venv = DummyVecEnv([lambda: StockTradingEnv(df, df2)])\n",
    "\n",
    "        learner = PPO(\n",
    "            env=venv,\n",
    "            policy=MlpPolicy,\n",
    "            batch_size=64,\n",
    "            ent_coef=0.0,\n",
    "            learning_rate=0.0003,\n",
    "            #learning_rate=0.0001,\n",
    "            n_epochs=20,\n",
    "        )\n",
    "\n",
    "        reward_net = BasicRewardNet(\n",
    "            venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    "        )\n",
    "\n",
    "        gail_trainer = GAIL(\n",
    "            demonstrations=[trajectory],\n",
    "            demo_batch_size=1024,\n",
    "            #demo_batch_size=32,\n",
    "            gen_replay_buffer_capacity=2048,\n",
    "            n_disc_updates_per_round=4,\n",
    "            venv=venv,\n",
    "            gen_algo=learner,\n",
    "            reward_net=reward_net,\n",
    "        )\n",
    "        \n",
    "        gail_trainer\n",
    "\n",
    "        #gail_trainer.train(1000000)\n",
    "        gail_trainer.train(2000000)\n",
    "        #gail_trainer.train(2048)\n",
    "        gail_trainer.gen_algo.save(\"./model3/Expert_\" + strategy + \"_\" + stock_id + \"_20_2M\")\n",
    "        \n",
    "        #gail_trainer.gen_algo.save(\"./model/Expert_\" + strategy + \"_\" + stock_id + \"_\" + str(day_length) + \"_train_nochip_2\")\n",
    "        #gail_trainer.gen_algo.save(\"./model/Expert_\" + strategy + \"_\" + stock_id + \"_\" + str(day_length) + \"_final\")\n",
    "\n",
    "        obs = venv.reset()\n",
    "        for i in range(len(df)):\n",
    "            action, _states = gail_trainer.gen_algo.predict(obs)\n",
    "            obs, rewards, done, info = venv.step(action)\n",
    "            venv.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定股票標的和開始/結束日期\n",
    "stock_list = ['2330', '2603', '2002','1301', '2801']\n",
    "stock_list = ['2603', '2002']\n",
    "strategy_list = ['SMA', 'KD', 'BBAND']\n",
    "#strategy_list = ['BBAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class SelfAttentionFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 220):\n",
    "        super(SelfAttentionFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(observation_space.shape[0], features_dim)\n",
    "        self.fc2 = nn.Linear(features_dim, features_dim)\n",
    "        self.attn = nn.MultiheadAttention(features_dim, num_heads=10)\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        #print(observations)\n",
    "        x = F.relu(self.fc1(observations))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x)\n",
    "        #print(x.shape)\n",
    "        x = x.unsqueeze(0)\n",
    "        #print(x.shape)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        #print(x)\n",
    "        return x.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "class SelfAttention(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 64):\n",
    "        super(SelfAttention, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        #self.batch_size = observation_space.shape[0]\n",
    "        #self.sequence_length = observation_space.shape[1]\n",
    "        self.sequence_length = 20\n",
    "        #self.input_dim = observation_space.shape[2]\n",
    "        self.input_dim = 11\n",
    "        self.hidden_dim = 64\n",
    "        \n",
    "        self.query_linear = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.key_linear = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.value_linear = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(self.hidden_dim, num_heads=8)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.hidden_dim*self.sequence_length, features_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, observations):\n",
    "        \n",
    "        #x = torch.Tensor(observations)\n",
    "        self.batch_size = observations.shape[0]\n",
    "        x = observations.view(self.batch_size, self.sequence_length, self.input_dim)\n",
    "        \n",
    "        # Query, Key, Value vectors\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "\n",
    "        # Multi-head attention\n",
    "        out, _ = self.multihead_attn(query.permute(1,0,2), key.permute(1,0,2), value.permute(1,0,2))\n",
    "\n",
    "        # Concatenate multi-head attention output\n",
    "        out = out.view(self.batch_size, -1)\n",
    "        \n",
    "        # Pass through fully-connected layers\n",
    "        out = self.linear1(out)\n",
    "        out = self.relu(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_id in stock_list:\n",
    "    for strategy in strategy_list:   \n",
    "        #stock_id = \"2330\"\n",
    "        #strategy = \"SMA\"\n",
    "        day_length = 20\n",
    "\n",
    "        start_date='2004-01-01'\n",
    "        end_date='2020-12-31'\n",
    "        #start_date='2013-01-01'\n",
    "        #start_date = '2021-01-01'\n",
    "        #end_date = '2021-12-31'\n",
    "\n",
    "\n",
    "        print(stock_id, strategy, day_length)\n",
    "\n",
    "        # 股價日成交資訊\n",
    "        df = api.taiwan_stock_daily(\n",
    "            stock_id = stock_id,\n",
    "            start_date = start_date,\n",
    "            end_date = end_date\n",
    "        )\n",
    "\n",
    "        #df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_' + str(day_length) + \"_nochip\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_30.csv',header=None)\n",
    "        df2 = pd.read_csv('./data_new/Input2/' + stock_id + '_input_train_20_old_price2.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/' + stock_id + '_input_train_' + str(day_length) + \"_NEW\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/' + stock_id + '_input_train_' + str(day_length) + \"_NEW\" + '.csv',header=None)\n",
    "        #df2 = pd.read_csv('./data/Input/temp/' + stock_id + '_input_train_' + str(day_length) + '.csv',header=None)\n",
    "        \n",
    "        print(len(df))\n",
    "        #print(df2)\n",
    "        print(len(df2[0]))\n",
    "\n",
    "        venv = DummyVecEnv([lambda: StockTradingEnv(df, df2)])\n",
    "\n",
    "        expert = PPO(\n",
    "            policy=MlpPolicy,\n",
    "            env=venv,\n",
    "            seed=0,\n",
    "            batch_size=64,\n",
    "            ent_coef=0.0,\n",
    "            learning_rate=0.0003,\n",
    "            n_epochs=10,\n",
    "            n_steps=64,\n",
    "        )\n",
    "\n",
    "        trajectory_list = []\n",
    "\n",
    "        filename = stock_id + '_' + strategy + '_trajectory_50_train_new.csv'\n",
    "        #filename = stock_id + '_' + strategy + '_trajectory_50_test.csv'\n",
    "\n",
    "        #filename = '2330_ZIGZAG_trajectory_0.02.csv'\n",
    "\n",
    "        #\"\"\"\n",
    "        with open(\"./data_new/Trajectory/Train/\" + filename, 'r', encoding='utf8', newline='') as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            for r in reader:\n",
    "                trajectory_list.append(r)\n",
    "        #\"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        with open(\"./data/Trajectory/Test/\" + filename, 'r', encoding='utf8', newline='') as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            for r in reader:\n",
    "                trajectory_list.append(r)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        final_trajectory = []\n",
    "        state = 0\n",
    "\n",
    "        # 0: all cash\n",
    "        # 1: all stock\n",
    "\n",
    "        for trajectory in trajectory_list:\n",
    "            if trajectory[1] == 'buy':\n",
    "                final_trajectory.append(1)\n",
    "                state = 1\n",
    "            elif trajectory[1] == 'sell':\n",
    "                final_trajectory.append(0)\n",
    "                state = 0\n",
    "            else:\n",
    "                if state == 0:\n",
    "                    final_trajectory.append(0)\n",
    "                else:\n",
    "                    final_trajectory.append(1)\n",
    "\n",
    "        shares_held = 0\n",
    "        reward_list = []\n",
    "\n",
    "        for i in range(len(final_trajectory)):\n",
    "            price = df.loc[i, \"close\"]\n",
    "            try:\n",
    "                previous_price = df.loc[i - 1, \"close\"]\n",
    "            except:\n",
    "                previous_price = df.loc[i, \"open\"]\n",
    "\n",
    "            if final_trajectory[i] == 0:\n",
    "                if shares_held > 0:\n",
    "                    sell_at_price = price\n",
    "                    shares_held = 0\n",
    "                    actual_action = 1\n",
    "                else:\n",
    "                    actual_action = 2\n",
    "            elif final_trajectory[i] == 1:\n",
    "                if shares_held == 0:\n",
    "                    buy_at_price = price\n",
    "                    shares_held = 1\n",
    "                    actual_action = 0\n",
    "                else:\n",
    "                    actual_action = 2\n",
    "\n",
    "            current_price = df.loc[i, \"close\"]\n",
    "\n",
    "\n",
    "            RR = (current_price - previous_price) / previous_price\n",
    "\n",
    "            if actual_action == 0:\n",
    "                # buy\n",
    "                reward = RR\n",
    "\n",
    "            elif actual_action == 1:\n",
    "                # sell\n",
    "                # 賣的時候就是看收益率/投資報酬率\n",
    "                RoR = (sell_at_price - buy_at_price) / buy_at_price\n",
    "                reward = RoR\n",
    "\n",
    "            elif actual_action == 2:\n",
    "                # hold\n",
    "                # RR正的時候盡量持有，負的時候就不要持有\n",
    "                reward = RR\n",
    "\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        t_action = np.array(final_trajectory)\n",
    "        t_reward = np.array(reward_list)\n",
    "\n",
    "        trajectory = types.TrajectoryWithRew(obs=df2, acts=t_action, infos=None, terminal=True, rews=t_reward)\n",
    "\n",
    "\n",
    "        venv = DummyVecEnv([lambda: StockTradingEnv(df, df2)])\n",
    "        #feature_extractor = SelfAttentionFeatureExtractor(venv.observation_space, hidden_size=64)\n",
    "        #print(observation_space)\n",
    "        \n",
    "        #policy_kwargs = dict(features_extractor_class=SelfAttentionFeatureExtractor2)\n",
    "        # Create the PPO model with the custom feature extractor\n",
    "        policy_kwargs=dict(features_extractor_class=SelfAttention, features_extractor_kwargs=dict(features_dim=220))\n",
    "        #ppo_model = PPO(\"MlpPolicy\", env, policy_kwargs=dict(features_extractor_class=SelfAttention, features_extractor_kwargs=dict(features_dim=220)))\n",
    "\n",
    "        learner = PPO(\n",
    "            env=venv,\n",
    "            policy=MlpPolicy,\n",
    "            batch_size=32,\n",
    "            ent_coef=0.0,\n",
    "            learning_rate=0.0001,\n",
    "            n_epochs=20,\n",
    "            \n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "        #print(learner.device)\n",
    "\n",
    "        reward_net = BasicRewardNet(\n",
    "            venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    "        )\n",
    "\n",
    "        gail_trainer = GAIL(\n",
    "            demonstrations=[trajectory],\n",
    "            #demo_batch_size=1024,\n",
    "            #gen_replay_buffer_capacity=2048,\n",
    "            demo_batch_size=16,\n",
    "            gen_replay_buffer_capacity=258,\n",
    "            n_disc_updates_per_round=4,\n",
    "            venv=venv,\n",
    "            gen_algo=learner,\n",
    "            reward_net=reward_net,\n",
    "        )\n",
    "        \n",
    "        gail_trainer\n",
    "\n",
    "        #gail_trainer.train(1000000)\n",
    "        gail_trainer.train(2000000)\n",
    "        #gail_trainer.train(4096)\n",
    "        gail_trainer.gen_algo.save(\"./model2/20_new/Expert_\" + strategy + \"_\" + stock_id + \"_20_2M_SA_8h\")\n",
    "        \n",
    "        #gail_trainer.gen_algo.save(\"./model/Expert_\" + strategy + \"_\" + stock_id + \"_\" + str(day_length) + \"_train_nochip_2\")\n",
    "        #gail_trainer.gen_algo.save(\"./model/Expert_\" + strategy + \"_\" + stock_id + \"_\" + str(day_length) + \"_final\")\n",
    "\n",
    "        obs = venv.reset()\n",
    "        for i in range(len(df)):\n",
    "            action, _states = gail_trainer.gen_algo.predict(obs)\n",
    "            obs, rewards, done, info = venv.step(action)\n",
    "            venv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "stock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
