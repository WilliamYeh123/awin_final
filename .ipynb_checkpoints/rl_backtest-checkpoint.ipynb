{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment forex-v0\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n",
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment stocks-v0\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "#from gym_trade.gym_anytrading.envs.stocks_env import StocksEnv\n",
    "#from env.ExpertEnv import StockTradingEnv\n",
    "import pandas as pd\n",
    "from FinMind.data import DataLoader\n",
    "#from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "#from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from gym import spaces\n",
    "#from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C,PPO,DQN\n",
    "#from stable_baselines import TRPO\n",
    "import gym_anytrading\n",
    "#from stable_baselines3 import PPO,DDPG\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from sb3_contrib.ppo_recurrent.ppo_recurrent import RecurrentPPO\n",
    "from sb3_contrib.trpo.trpo import TRPO\n",
    "from sb3_contrib.qrdqn.qrdqn import QRDQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import talib\n",
    "import torch\n",
    "\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "from gym_trade.gym_anytrading.envs import StocksEnv as StocksEnv2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "api_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJkYXRlIjoiMjAyMS0xMi0yNyAxNDo1OTowOSIsInVzZXJfaWQiOiJkdXJhbnQ3MTA5MTYiLCJpcCI6IjE0MC4xMjAuMTMuMjMwIn0.8-KIC3-OA4D6JcOtQ_fJBOVkyugx60t1Gy82c57TLz4\"\n",
    "\n",
    "api = DataLoader()\n",
    "api.login_by_token(api_token = api_token)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_A2C(env_train, model_name, timesteps=50000):\n",
    "    \"\"\"A2C model\"\"\"\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=5, min_evals=5, verbose=0)\n",
    "    eval_callback = EvalCallback(env_train, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=0)\n",
    "\n",
    "    start = time.time()\n",
    "    #model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    model = A2C('MlpPolicy', env_train, verbose=0, ent_coef=0.1)\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "    end = time.time()\n",
    "\n",
    "    #model.save(f\"{config.TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    #model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "'''\n",
    "def train_DDPG(env_train, model_name, timesteps=200000):\n",
    "    \"\"\"DDPG model\"\"\"\n",
    "\n",
    "    # add the noise objects for DDPG\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    param_noise = None\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "    start = time.time()\n",
    "    #model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n",
    "    model = DDPG('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (DDPG): ', (end-start)/60,' minutes')\n",
    "    return model\n",
    "'''\n",
    "def train_PPO(env_train, model_name, timesteps=200000):\n",
    "    \"\"\"PPO model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=0)\n",
    "    eval_callback = EvalCallback(env_train, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=0)\n",
    "\n",
    "    model = PPO('MlpPolicy', env_train, verbose=0, ent_coef=0.2,batch_size=64)\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "def train_DQN(env_train, model_name, timesteps=50000):\n",
    "    \"\"\"DQN model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    #model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=0)\n",
    "    eval_callback = EvalCallback(env_train, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=0)\n",
    "\n",
    "    model = DQN('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "    end = time.time()\n",
    "\n",
    "    #model.save(f\"{config.TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    #model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (DQN): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_DRQN(env_train, model_name, timesteps=50000):\n",
    "    \"\"\"TRPO model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=0)\n",
    "    eval_callback = EvalCallback(env_train, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=0)\n",
    "\n",
    "    model = RecurrentPPO('MlpLstmPolicy', env_train, verbose=0, ent_coef=0.1)\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "    end = time.time()\n",
    "\n",
    "    #model.save(f\"{config.TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    #model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (DRQN): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_TRPO(env_train, model_name, timesteps=50000):\n",
    "    \"\"\"TRPO model\"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=0)\n",
    "    eval_callback = EvalCallback(env_train, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=0)\n",
    "    \n",
    "    #model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    model = TRPO('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "    end = time.time()\n",
    "\n",
    "    #model.save(f\"{config.TRAINED_MODEL_DIR}/{model_name}\")\n",
    "    #model.save(\"./expert_model/\" + model_name)\n",
    "    print('Training time (TRPO): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "def sharpeRatio(Ret):\n",
    "    T = len(Ret)\n",
    "    if T == 0:\n",
    "        return 0\n",
    "    mean_ret = float(sum(Ret))/T\n",
    "    mean_sq_ret = float(sum(Ret**2))/T\n",
    "    if (mean_ret == 0.0) & (mean_sq_ret == 0.0):\n",
    "        return 0\n",
    "    if mean_sq_ret - mean_ret*mean_ret == 0:\n",
    "        return 0\n",
    "    sharpe = mean_ret/sqrt(mean_sq_ret - mean_ret*mean_ret)\n",
    "    return sharpe\n",
    "def DRL_validation(df ,model, test_env, test_obs) -> None:\n",
    "    ###validation process###\n",
    "    #env_val2 = MyCustomEnv(df=df, frame_bound=(val_start_idx,val_end_idx), window_size=20)\n",
    "    #obs_val2 = env_val2.reset()\n",
    "    \n",
    "    action_list = []\n",
    "        \n",
    "    final_action = []\n",
    "    hold = 0\n",
    "    temp = []\n",
    "    buy_price = 0\n",
    "    #prices = []\n",
    "    return_list = []\n",
    "    Rf = 0.02   # 假設無風險利率為2%\n",
    "    downside_deviation = []\n",
    "    action_count=0\n",
    "    while True: \n",
    "        test_obs = test_obs[np.newaxis, ...]\n",
    "        #print(test_obs)\n",
    "        price = test_obs[0][-1][0]\n",
    "        #print(price)\n",
    "        #print(obs.shape)\n",
    "        action, _states = model.predict(test_obs)\n",
    "        #print(action[0])\n",
    "        action_count+=1\n",
    "        if action[0] == 1:\n",
    "            if hold == 0:\n",
    "                print('buy ',price)\n",
    "                #buy\n",
    "                buy_price = price\n",
    "                hold = 1\n",
    "           \n",
    "                #print('hold')\n",
    "        elif action[0] == 0:\n",
    "            if hold == 1:\n",
    "                #sell\n",
    "                print('sell ',price)\n",
    "                return_list.append((price-buy_price)/buy_price)\n",
    "                hold = 0\n",
    "            \n",
    "                #print('hold')\n",
    "        \n",
    "        #action_list.append(action[0])\n",
    "        #obs, rewards, dones, info = env.step(action)\n",
    "        test_obs, rewards, done, info = test_env.step(action)\n",
    "        if done:\n",
    "            print(\"info\", info)\n",
    "            break\n",
    "    print(return_list)\n",
    "    print(len(return_list))\n",
    "    #print(action_count)\n",
    "    \n",
    "    for r in return_list:\n",
    "        if r-Rf<=0:\n",
    "            downside_deviation.append(0)\n",
    "        else:\n",
    "            downside_deviation.append(r-Rf)\n",
    "    \n",
    "    #Sharpe Ratio\n",
    "    #Rp = (1 + np.mean(return_list)) ** (action_count / len(return_list)) - 1\n",
    "    Rp = np.mean(return_list)\n",
    "    #print(Rp)\n",
    "    #print(Rp)\n",
    "    # 確定無風險利率\n",
    "    \n",
    "    # 計算標準差\n",
    "    sigma_p = np.std(return_list)\n",
    "    #print(sigma_p)\n",
    "    \n",
    "    # 計算Sharpe Ratio\n",
    "    Sharpe_ratio = (Rp - Rf) / sigma_p\n",
    "    Sortino_ratio = (Rp - Rf) / np.std(downside_deviation)\n",
    "    Sharpe_ratio = sharpeRatio(np.array(return_list))\n",
    "    return Sharpe_ratio, Sortino_ratio\n",
    "def DRL_prediction(df,\n",
    "                   model,\n",
    "                   #name,\n",
    "                   #last_state,\n",
    "                   start_date,\n",
    "                   end_date,\n",
    "                   action_list\n",
    "                  ):\n",
    "    ### make a prediction based on trained model###\n",
    "\n",
    "    ## trading env\n",
    "    trade_data = df[start_date:end_date]\n",
    "    env_trade = MyCustomEnv(df=df, frame_bound=(start_date,end_date+1), window_size=20)\n",
    "    \n",
    "    obs = env_trade.reset()\n",
    "    while True: \n",
    "        #obs = obs[np.newaxis, ...]\n",
    "        #print(obs[-1][0])\n",
    "        action, _states = model.predict(obs)\n",
    "        action_list.append(action)\n",
    "        #print(action)\n",
    "        obs, rewards, done, info = env_trade.step(action)\n",
    "        if done:\n",
    "            print(\"info\", info)\n",
    "            break\n",
    "\n",
    "    return action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "start_date='1997-10-18'\n",
    "end_date='2020-12-31'\n",
    "df = api.taiwan_stock_daily(\n",
    "            stock_id = '2330',\n",
    "            start_date = start_date,\n",
    "            end_date = end_date\n",
    ")\n",
    "\n",
    "df = df.iloc[:][['date','open', 'max', 'min', 'close','Trading_Volume']]\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.rename(columns = {'date':'Date', 'open':'Open','max':'High','min':'Low', 'close':'Close','Trading_Volume':'Volume'}, inplace = True)\n",
    "dateArray = df['Date']\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "max_value = 0.8\n",
    "min_value = 0.2\n",
    "\n",
    "Max_Volume = df['Volume'].max()\n",
    "Min_Volume = df['Volume'].min()\n",
    "Max_Price = df['High'].max()\n",
    "Min_Price = df['Low'].min()\n",
    "\n",
    "df['RSI'] = talib.RSI(df.iloc[:]['Close'], timeperiod=14)/100\n",
    "df['EMA'] = talib.EMA(df.iloc[:]['Close'], timeperiod=10)\n",
    "df['OBV'] = talib.OBV(df.iloc[:]['Close'], df.iloc[:]['Volume'])\n",
    "Max_OBV = df['OBV'].max()\n",
    "Min_OBV = df['OBV'].min()\n",
    "\n",
    "df['High'] = min_value + (max_value - min_value) * (df['High'] - Min_Price) / (Max_Price - Min_Price)\n",
    "df['Low'] = min_value + (max_value - min_value) * (df['Low'] - Min_Price) / (Max_Price - Min_Price)\n",
    "df['Open'] = min_value + (max_value - min_value) * (df['Open'] - Min_Price) / (Max_Price - Min_Price)\n",
    "df['Close'] = min_value + (max_value - min_value) * (df['Close'] - Min_Price) / (Max_Price - Min_Price)\n",
    "df['Volume'] = min_value + (max_value - min_value) * (df['Volume'] - Min_Volume) / (Max_Volume - Min_Volume)\n",
    "\n",
    "df['EMA'] = min_value + (max_value - min_value) * (df['EMA'] - Min_Price) / (Max_Price - Min_Price)\n",
    "df['OBV'] = min_value + (max_value - min_value) * (df['OBV'] - Min_OBV) / (Max_OBV - Min_OBV)\n",
    "\n",
    "\n",
    "\n",
    "train_start = '2000-01-01'\n",
    "train_start = datetime.strptime(train_start, '%Y-%m-%d')\n",
    "train_start_idx = df.index.get_loc(dateArray[dateArray>=train_start].iloc[0])\n",
    "train_end = '2003-01-01'\n",
    "train_end = datetime.strptime(train_end, '%Y-%m-%d')\n",
    "train_end_idx = df.index.get_loc(dateArray[dateArray<train_end].iloc[-1])\n",
    "val_start = '2003-01-01'\n",
    "val_start = datetime.strptime(val_start, '%Y-%m-%d')\n",
    "val_start_idx = df.index.get_loc(dateArray[dateArray>=val_start].iloc[0])\n",
    "val_end = '2004-1-1'\n",
    "val_end = datetime.strptime(val_end, '%Y-%m-%d')\n",
    "val_end_idx = df.index.get_loc(dateArray[dateArray<val_end].iloc[-1])\n",
    "test_start = '2004-01-01'\n",
    "test_start = datetime.strptime(test_start, '%Y-%m-%d')\n",
    "test_start_idx = df.index.get_loc(dateArray[dateArray>=test_start].iloc[0])\n",
    "test_end = '2005-1-1'\n",
    "test_end = datetime.strptime(test_end, '%Y-%m-%d')\n",
    "test_end_idx = df.index.get_loc(dateArray[dateArray<test_end].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: StocksEnv2(df=df, frame_bound=(train_start_idx,train_end_idx), window_size=30)]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:71: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-388.95 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-1506.80 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-3000.91 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-5573.71 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2486.44 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-2725.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-257.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=-3006.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-3006.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-1158.79 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-139.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-881.44 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-2790.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1352.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-791.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=-2757.39 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-783.02 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-235.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-267.46 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-1051.09 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-3021.40 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-204.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-1351.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-1375.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-1534.52 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-652.52 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-182.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-1758.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-2790.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-2790.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-198.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-229.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-265.07 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-214.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-2624.90 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=360000, episode_reward=-630.81 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-1533.85 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-1312.92 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=376000, episode_reward=-133.71 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-1326.78 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-1098.59 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-679.89 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-42.47 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=-1937.31 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=-205.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-1927.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-1096.46 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-372.01 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-413.50 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-41.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=432000, episode_reward=-37.35 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=436000, episode_reward=-1955.34 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-483.47 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-81.84 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-76.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-43.44 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-50.14 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-2048.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-457.82 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-2370.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-1211.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-758.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-52.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-339.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-674.72 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-199.59 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-678.37 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7fbe36ae7b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, verbose=1)\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=0, ent_coef=0.01,n_steps=10)\n",
    "model.learn(total_timesteps=500000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awinlab/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:71: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c9cc38d36960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment_coef\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# Normalize advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mmin_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mp_log_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mp_log_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, verbose=1)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0, ent_coef=0.1)\n",
    "model.learn(total_timesteps=400000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-0.77 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=0.48 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=64000, episode_reward=-0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-0.76 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-0.42 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=88000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=-0.38 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=136000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=-0.62 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=-0.38 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=-0.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=-0.44 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=204000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=212000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=-0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=228000, episode_reward=-0.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=-0.38 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-0.57 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=244000, episode_reward=-0.55 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=-0.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=-0.30 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=-0.26 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=-0.63 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=-0.55 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=292000, episode_reward=-0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-0.54 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=-0.38 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=-0.64 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=-0.61 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=-0.40 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=-0.57 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=-0.58 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=-0.48 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-0.39 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=-0.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=-0.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=-0.32 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=376000, episode_reward=-0.64 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-0.25 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=384000, episode_reward=-0.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=-0.30 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=-0.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=408000, episode_reward=-0.39 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=416000, episode_reward=-0.57 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=424000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=432000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=-0.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=-0.30 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=-0.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=464000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=-0.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=-0.59 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=488000, episode_reward=-0.33 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=-0.34 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-0.40 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=-0.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=-0.31 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=-0.21 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-0.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=524000, episode_reward=-0.46 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=-0.40 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=-0.43 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=-0.36 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-0.53 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=548000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=-0.50 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=556000, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-0.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=-0.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=-0.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=572000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=-0.67 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-0.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=-0.48 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=588000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=-0.37 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f03130af290>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, verbose=1)\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=0,learning_rate=0.0003)\n",
    "model.learn(total_timesteps=600000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.113    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 28777    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2956     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 28067    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5912     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 27936    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8868     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 27863    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11824    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 27817    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 14780    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 27889    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 17736    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 27827    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 20692    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 27863    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 23648    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 27899    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 26604    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 27919    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 29560    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 27931    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 32516    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 27911    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 35472    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 27922    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 38428    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 27928    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 41384    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 27906    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 44340    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 27926    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 47296    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 27000    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 50252    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 106      |\n",
      "|    n_updates        | 15       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 19928    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 53208    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 63.1     |\n",
      "|    n_updates        | 200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 16193    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 56164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 61.4     |\n",
      "|    n_updates        | 385      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 13972    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 59120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 60.9     |\n",
      "|    n_updates        | 569      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 12299    |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 62076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 21.7     |\n",
      "|    n_updates        | 754      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 11101    |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 65032    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 25.1     |\n",
      "|    n_updates        | 939      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 10226    |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 67988    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 19.8     |\n",
      "|    n_updates        | 1124     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 9507     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 70944    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 6.16     |\n",
      "|    n_updates        | 1308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 8938     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 73900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 5.01     |\n",
      "|    n_updates        | 1493     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 8476     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 76856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 6.06     |\n",
      "|    n_updates        | 1678     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 8093     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 79812    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 5.53     |\n",
      "|    n_updates        | 1863     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 7762     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 82768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 1.35     |\n",
      "|    n_updates        | 2047     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 7495     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 85724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 2232     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 7248     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 88680    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 2417     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 7016     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 91636    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.442    |\n",
      "|    n_updates        | 2602     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 6812     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 94592    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.365    |\n",
      "|    n_updates        | 2786     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 6640     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 97548    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.378    |\n",
      "|    n_updates        | 2971     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 6484     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 100504   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 3156     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 6354     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 103460   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.299    |\n",
      "|    n_updates        | 3341     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 6228     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 106416   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 3525     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 6123     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 109372   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.186    |\n",
      "|    n_updates        | 3710     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 6018     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 112328   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 3895     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 5924     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 115284   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 4080     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 5837     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 118240   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 4264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 5755     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 121196   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.125    |\n",
      "|    n_updates        | 4449     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 5680     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 124152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 4634     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 5597     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 127108   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.121    |\n",
      "|    n_updates        | 4819     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 5525     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 130064   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0761   |\n",
      "|    n_updates        | 5003     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 5458     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 133020   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.157    |\n",
      "|    n_updates        | 5188     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 5409     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 135976   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0444   |\n",
      "|    n_updates        | 5373     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 5368     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 138932   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 5558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 5330     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 141888   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0964   |\n",
      "|    n_updates        | 5742     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 5294     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 144844   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0962   |\n",
      "|    n_updates        | 5927     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 5260     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 147800   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.122    |\n",
      "|    n_updates        | 6112     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 5228     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 150756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0791   |\n",
      "|    n_updates        | 6297     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 5199     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 153712   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.154    |\n",
      "|    n_updates        | 6481     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 5171     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 156668   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 6666     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 5144     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 159624   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.235    |\n",
      "|    n_updates        | 6851     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 5118     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 162580   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0751   |\n",
      "|    n_updates        | 7036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 5093     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 165536   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0781   |\n",
      "|    n_updates        | 7220     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 5068     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 168492   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0922   |\n",
      "|    n_updates        | 7405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 5045     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 171448   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 7590     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 5023     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 174404   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0843   |\n",
      "|    n_updates        | 7775     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 5000     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 177360   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0934   |\n",
      "|    n_updates        | 7959     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 4952     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 180316   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0712   |\n",
      "|    n_updates        | 8144     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 4863     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 183272   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 8329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 4763     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 186228   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 8514     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 4713     |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 189184   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0448   |\n",
      "|    n_updates        | 8698     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 4667     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 192140   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.113    |\n",
      "|    n_updates        | 8883     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 4621     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 195096   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.123    |\n",
      "|    n_updates        | 9068     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 4582     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 198052   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0407   |\n",
      "|    n_updates        | 9253     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 4541     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 201008   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0995   |\n",
      "|    n_updates        | 9437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 4509     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 203964   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0945   |\n",
      "|    n_updates        | 9622     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 4495     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 206920   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.241    |\n",
      "|    n_updates        | 9807     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 4486     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 209876   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 9992     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 4478     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 212832   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.166    |\n",
      "|    n_updates        | 10176    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 4469     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 215788   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0573   |\n",
      "|    n_updates        | 10361    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 4459     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 218744   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0782   |\n",
      "|    n_updates        | 10546    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 4440     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 221700   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 10731    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 4394     |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 224656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0611   |\n",
      "|    n_updates        | 10915    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 4331     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 227612   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 11100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 4297     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 230568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 11285    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 4278     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 233524   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 11470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 4268     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 236480   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0709   |\n",
      "|    n_updates        | 11654    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 4260     |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 239436   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.117    |\n",
      "|    n_updates        | 11839    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 4252     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 242392   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 12024    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 4242     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 245348   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 12209    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 4232     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 248304   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0418   |\n",
      "|    n_updates        | 12393    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 4224     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 251260   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.04     |\n",
      "|    n_updates        | 12578    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 4217     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 254216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.14     |\n",
      "|    n_updates        | 12763    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 4210     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 257172   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0765   |\n",
      "|    n_updates        | 12948    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 4202     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 260128   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0887   |\n",
      "|    n_updates        | 13132    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 4196     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 263084   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0743   |\n",
      "|    n_updates        | 13317    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 4189     |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 266040   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0293   |\n",
      "|    n_updates        | 13502    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 4179     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 268996   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0465   |\n",
      "|    n_updates        | 13687    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 4172     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 271952   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0246   |\n",
      "|    n_updates        | 13871    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 4165     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 274908   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 14056    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 4159     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 277864   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.078    |\n",
      "|    n_updates        | 14241    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 4153     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 280820   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0641   |\n",
      "|    n_updates        | 14426    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 4146     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 283776   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0804   |\n",
      "|    n_updates        | 14610    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 4140     |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 286732   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 14795    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 4135     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 289688   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 14980    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 4131     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 292644   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 15165    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 4127     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 295600   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.105    |\n",
      "|    n_updates        | 15349    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 4121     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 298556   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0532   |\n",
      "|    n_updates        | 15534    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 4117     |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 301512   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0445   |\n",
      "|    n_updates        | 15719    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 4112     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 304468   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.359    |\n",
      "|    n_updates        | 15904    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 4108     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 307424   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0715   |\n",
      "|    n_updates        | 16088    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 4105     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 310380   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0307   |\n",
      "|    n_updates        | 16273    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 4100     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 313336   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 16458    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 4096     |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 316292   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 16643    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 4091     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 319248   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 16827    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 4084     |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 322204   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0945   |\n",
      "|    n_updates        | 17012    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 4079     |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 325160   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.095    |\n",
      "|    n_updates        | 17197    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 4074     |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 328116   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.033    |\n",
      "|    n_updates        | 17382    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 4069     |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 331072   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0744   |\n",
      "|    n_updates        | 17566    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 4065     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 334028   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0639   |\n",
      "|    n_updates        | 17751    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 4060     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 336984   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.204    |\n",
      "|    n_updates        | 17936    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 4056     |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 339940   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0208   |\n",
      "|    n_updates        | 18121    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 4052     |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 342896   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 18305    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 4047     |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 345852   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.141    |\n",
      "|    n_updates        | 18490    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 4044     |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 348808   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0832   |\n",
      "|    n_updates        | 18675    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 4039     |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 351764   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0284   |\n",
      "|    n_updates        | 18860    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 4034     |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 354720   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0528   |\n",
      "|    n_updates        | 19044    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 4030     |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 357676   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0958   |\n",
      "|    n_updates        | 19229    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 4026     |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 360632   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0529   |\n",
      "|    n_updates        | 19414    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 4023     |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 363588   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 19599    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 4020     |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 366544   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.074    |\n",
      "|    n_updates        | 19783    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 4017     |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 369500   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0798   |\n",
      "|    n_updates        | 19968    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 4013     |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 372456   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0794   |\n",
      "|    n_updates        | 20153    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 4010     |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 375412   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 20338    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 4006     |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 378368   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0658   |\n",
      "|    n_updates        | 20522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 4000     |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 381324   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0387   |\n",
      "|    n_updates        | 20707    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 3996     |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 384280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0351   |\n",
      "|    n_updates        | 20892    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 3992     |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 387236   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0286   |\n",
      "|    n_updates        | 21077    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 3989     |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 390192   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0384   |\n",
      "|    n_updates        | 21261    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 3986     |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 393148   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 21446    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 3984     |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 396104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 21631    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 3982     |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 399060   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0821   |\n",
      "|    n_updates        | 21816    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 3979     |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 402016   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 22000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 3977     |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 404972   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0582   |\n",
      "|    n_updates        | 22185    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 3975     |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 407928   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0479   |\n",
      "|    n_updates        | 22370    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 3973     |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 410884   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0622   |\n",
      "|    n_updates        | 22555    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 3970     |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 413840   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0469   |\n",
      "|    n_updates        | 22739    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 3966     |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 416796   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.054    |\n",
      "|    n_updates        | 22924    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 3960     |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 419752   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0431   |\n",
      "|    n_updates        | 23109    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 3956     |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 422708   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 23294    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 3953     |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 425664   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0528   |\n",
      "|    n_updates        | 23478    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 3950     |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 428620   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0332   |\n",
      "|    n_updates        | 23663    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 3948     |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 431576   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0602   |\n",
      "|    n_updates        | 23848    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 3944     |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 434532   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0568   |\n",
      "|    n_updates        | 24033    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 3941     |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 437488   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.28     |\n",
      "|    n_updates        | 24217    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 3938     |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 440444   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0229   |\n",
      "|    n_updates        | 24402    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 3935     |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 443400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0886   |\n",
      "|    n_updates        | 24587    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 3932     |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 446356   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 24772    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 3929     |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 449312   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.028    |\n",
      "|    n_updates        | 24956    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 3926     |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 452268   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0553   |\n",
      "|    n_updates        | 25141    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 3924     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 455224   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0455   |\n",
      "|    n_updates        | 25326    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 3920     |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 458180   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.00842  |\n",
      "|    n_updates        | 25511    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 3916     |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 461136   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0616   |\n",
      "|    n_updates        | 25695    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 3915     |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 464092   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0627   |\n",
      "|    n_updates        | 25880    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 3912     |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 467048   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0276   |\n",
      "|    n_updates        | 26065    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 3909     |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 470004   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0531   |\n",
      "|    n_updates        | 26250    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 3907     |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 472960   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 26434    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 3905     |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 475916   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0811   |\n",
      "|    n_updates        | 26619    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 3903     |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 478872   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0254   |\n",
      "|    n_updates        | 26804    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 3901     |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 481828   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0683   |\n",
      "|    n_updates        | 26989    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 3899     |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 484784   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0909   |\n",
      "|    n_updates        | 27173    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 3897     |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 487740   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 27358    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 3895     |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 490696   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0406   |\n",
      "|    n_updates        | 27543    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 3893     |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 493652   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0401   |\n",
      "|    n_updates        | 27728    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 3891     |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 496608   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 27912    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 3888     |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 499564   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 28097    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 3887     |\n",
      "|    time_elapsed     | 129      |\n",
      "|    total_timesteps  | 502520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.211    |\n",
      "|    n_updates        | 28282    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 3885     |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 505476   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0812   |\n",
      "|    n_updates        | 28467    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 3882     |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 508432   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0531   |\n",
      "|    n_updates        | 28651    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 3880     |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 511388   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 28836    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 3878     |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 514344   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0565   |\n",
      "|    n_updates        | 29021    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 3876     |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 517300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.105    |\n",
      "|    n_updates        | 29206    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 3874     |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 520256   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0526   |\n",
      "|    n_updates        | 29390    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 3873     |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 523212   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.023    |\n",
      "|    n_updates        | 29575    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 3872     |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 526168   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.71     |\n",
      "|    n_updates        | 29760    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 3870     |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 529124   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 29945    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 3868     |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 532080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0344   |\n",
      "|    n_updates        | 30129    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 3867     |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 535036   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 30314    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 3866     |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 537992   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0286   |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 3866     |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 540948   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0494   |\n",
      "|    n_updates        | 30684    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 3864     |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 543904   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0343   |\n",
      "|    n_updates        | 30868    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 3862     |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 546860   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0686   |\n",
      "|    n_updates        | 31053    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 3860     |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 549816   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0789   |\n",
      "|    n_updates        | 31238    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 3859     |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 552772   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 31423    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 3858     |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 555728   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.109    |\n",
      "|    n_updates        | 31607    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 3857     |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 558684   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.188    |\n",
      "|    n_updates        | 31792    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 3856     |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 561640   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 31977    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 3854     |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 564596   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0707   |\n",
      "|    n_updates        | 32162    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 3852     |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 567552   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 32346    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 3851     |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 570508   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0603   |\n",
      "|    n_updates        | 32531    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 3850     |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 573464   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0333   |\n",
      "|    n_updates        | 32716    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 3850     |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 576420   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0376   |\n",
      "|    n_updates        | 32901    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 3849     |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 579376   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0132   |\n",
      "|    n_updates        | 33085    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 3847     |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 582332   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.117    |\n",
      "|    n_updates        | 33270    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 3846     |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 585288   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.118    |\n",
      "|    n_updates        | 33455    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 3844     |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 588244   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0508   |\n",
      "|    n_updates        | 33640    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 3843     |\n",
      "|    time_elapsed     | 153      |\n",
      "|    total_timesteps  | 591200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 33824    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 3842     |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 594156   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0262   |\n",
      "|    n_updates        | 34009    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 3841     |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 597112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 34194    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.qrdqn.qrdqn.QRDQN at 0x7f6d104ebcd0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=20, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "model = QRDQN('MlpPolicy', env, verbose=1,learning_rate=0.0003, gamma=0.6,exploration_final_eps=0.1)\n",
    "model.learn(total_timesteps=600000, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO at 0x7ff3d5c032d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "model = RecurrentPPO('MlpLstmPolicy', env, verbose=0, ent_coef=0.01,batch_size = 64, n_epochs=20)\n",
    "model.learn(total_timesteps=600000, callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=-2790.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-2790.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-5618.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-533.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=-533.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-883.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-883.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-136.12 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-136.12 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-227.85 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-227.85 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=52000, episode_reward=-321.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-321.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-49.83 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=-49.83 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=68000, episode_reward=-50.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=72000, episode_reward=-50.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=76000, episode_reward=-3.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-3.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=84000, episode_reward=-2.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=88000, episode_reward=-2.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=92000, episode_reward=-3.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=-3.00 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2.85 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=104000, episode_reward=-2.85 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=108000, episode_reward=0.15 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112000, episode_reward=0.15 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=116000, episode_reward=1.06 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=1.06 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=124000, episode_reward=0.35 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=0.35 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=132000, episode_reward=3.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=136000, episode_reward=3.24 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=2.68 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=2.68 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=148000, episode_reward=2.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=152000, episode_reward=2.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=156000, episode_reward=2.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=2.05 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=164000, episode_reward=3.13 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=168000, episode_reward=3.13 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=172000, episode_reward=3.13 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=3.83 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=3.83 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=184000, episode_reward=3.78 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=188000, episode_reward=3.78 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=3.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=196000, episode_reward=3.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=4.02 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=204000, episode_reward=4.02 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=208000, episode_reward=4.07 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=212000, episode_reward=4.07 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=216000, episode_reward=4.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=4.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=224000, episode_reward=4.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=228000, episode_reward=4.18 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=232000, episode_reward=4.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=236000, episode_reward=4.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=4.49 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=244000, episode_reward=4.49 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=248000, episode_reward=4.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=252000, episode_reward=4.22 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=256000, episode_reward=4.19 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=4.19 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=264000, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=268000, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=272000, episode_reward=4.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=276000, episode_reward=4.20 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=4.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=284000, episode_reward=4.23 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=288000, episode_reward=4.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=292000, episode_reward=4.56 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=296000, episode_reward=4.78 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=4.78 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=304000, episode_reward=4.76 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=308000, episode_reward=4.76 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=312000, episode_reward=4.75 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=316000, episode_reward=4.75 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=4.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=324000, episode_reward=4.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=328000, episode_reward=4.74 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=332000, episode_reward=4.74 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=336000, episode_reward=4.88 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=340000, episode_reward=4.88 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=344000, episode_reward=4.88 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=348000, episode_reward=4.84 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=352000, episode_reward=4.84 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=356000, episode_reward=4.76 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=360000, episode_reward=4.76 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=364000, episode_reward=4.81 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=368000, episode_reward=4.81 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=372000, episode_reward=4.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=376000, episode_reward=4.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=4.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=384000, episode_reward=4.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=388000, episode_reward=4.97 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=392000, episode_reward=4.97 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=396000, episode_reward=5.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=5.10 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=404000, episode_reward=5.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=408000, episode_reward=5.11 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=412000, episode_reward=5.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=416000, episode_reward=5.16 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=424000, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=428000, episode_reward=5.21 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=432000, episode_reward=5.21 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=436000, episode_reward=5.31 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=5.31 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=444000, episode_reward=5.28 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=448000, episode_reward=5.28 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=452000, episode_reward=5.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=456000, episode_reward=5.29 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=464000, episode_reward=5.33 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=468000, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=472000, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=476000, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=5.26 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=484000, episode_reward=5.59 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=488000, episode_reward=5.59 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=492000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=496000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=504000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=508000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=512000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=516000, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=524000, episode_reward=5.60 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=528000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=532000, episode_reward=5.58 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=536000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=540000, episode_reward=5.67 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=544000, episode_reward=5.68 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=548000, episode_reward=5.68 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=552000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=556000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=564000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=568000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=572000, episode_reward=5.70 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=576000, episode_reward=5.72 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=580000, episode_reward=5.72 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=584000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=588000, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=592000, episode_reward=5.73 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=596000, episode_reward=5.73 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=5.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=604000, episode_reward=5.65 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=608000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=612000, episode_reward=5.69 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=616000, episode_reward=5.91 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=620000, episode_reward=5.91 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=624000, episode_reward=5.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=628000, episode_reward=5.87 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=632000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=636000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=5.95 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=644000, episode_reward=5.95 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=648000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=652000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=656000, episode_reward=5.92 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=5.92 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=664000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=668000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=672000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=676000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=684000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=688000, episode_reward=5.98 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=692000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=696000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=704000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=708000, episode_reward=5.95 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=712000, episode_reward=5.95 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=716000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=724000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=728000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=732000, episode_reward=5.91 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=736000, episode_reward=5.91 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=744000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=748000, episode_reward=5.92 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=752000, episode_reward=5.92 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=756000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=5.93 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=764000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=768000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=772000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=776000, episode_reward=5.96 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=5.94 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=784000, episode_reward=5.94 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=788000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=792000, episode_reward=5.89 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=796000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=5.99 +/- 0.00\n",
      "Episode length: 761.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-476283b34ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcg_max_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/sb3_contrib/trpo/trpo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_features_extractor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mpi_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[1;32m    262\u001b[0m         \u001b[0mshared_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stock/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=20, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, verbose=1)\n",
    "\n",
    "model = TRPO('MlpPolicy', env, verbose=0, cg_max_steps=30)\n",
    "\n",
    "model.learn(total_timesteps=1000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy  0.23465966471419916\n",
      "sell  0.23647747929711171\n",
      "buy  0.2370834174914159\n",
      "sell  0.23829529388002424\n",
      "buy  0.23890123207432845\n",
      "sell  0.23829529388002424\n",
      "buy  0.23526560290850335\n",
      "sell  0.23526560290850335\n",
      "buy  0.2334477883255908\n",
      "sell  0.23405372651989498\n",
      "buy  0.23647747929711171\n",
      "sell  0.2370834174914159\n",
      "buy  0.23768935568572008\n",
      "sell  0.23587154110280753\n",
      "buy  0.22738840638254898\n",
      "sell  0.22981215915976572\n",
      "buy  0.22860028277115735\n",
      "sell  0.23162997374267827\n",
      "buy  0.23162997374267827\n",
      "sell  0.2328418501312866\n",
      "buy  0.22860028277115735\n",
      "sell  0.22496465360533227\n",
      "buy  0.22678246818824482\n",
      "sell  0.2237527772167239\n",
      "buy  0.22254090082811553\n",
      "sell  0.22254090082811553\n",
      "buy  0.22557059179963646\n",
      "sell  0.22557059179963646\n",
      "buy  0.2243587154110281\n",
      "sell  0.2243587154110281\n",
      "buy  0.21017976166431024\n",
      "sell  0.21005857402544942\n",
      "buy  0.2130882649969703\n",
      "sell  0.21175520096950112\n",
      "buy  0.2113916380529186\n",
      "sell  0.21211876388608364\n",
      "buy  0.20945263583114523\n",
      "sell  0.2072712583316502\n",
      "buy  0.207392445970511\n",
      "sell  0.2096950111088669\n",
      "buy  0.21005857402544942\n",
      "sell  0.2103009493031711\n",
      "buy  0.2129670773581095\n",
      "sell  0.21393657846899616\n",
      "buy  0.21587558069076956\n",
      "sell  0.21272470208038782\n",
      "buy  0.21078569985861442\n",
      "sell  0.21078569985861442\n",
      "buy  0.21260351444152698\n",
      "sell  0.21248232680266615\n",
      "buy  0.2119975762472228\n",
      "sell  0.2119975762472228\n",
      "buy  0.21054332458089275\n",
      "sell  0.2123611391638053\n",
      "buy  0.20921026055342357\n",
      "sell  0.21005857402544942\n",
      "buy  0.21017976166431024\n",
      "sell  0.21611795596849123\n",
      "buy  0.21623914360735205\n",
      "sell  0.2136942031912745\n",
      "buy  0.2146637043021612\n",
      "sell  0.2146637043021612\n",
      "buy  0.21551201777418705\n",
      "sell  0.21757220763482127\n",
      "buy  0.21757220763482127\n",
      "sell  0.21781458291254294\n",
      "info {'total_reward': 0.10965664033116694, 'total_profit': 0.5728243137450268, 'position': 0}\n",
      "[0.0077466001032880415, 0.005111603339580758, -0.0025363544132568083, 0.0, 0.0025956047759128015, 0.002562350529552456, -0.007647858599592153, 0.01065908687155806, 0.01325322495140491, 0.005231949773282084, -0.015903869941685772, -0.013359458496615673, 0.0, 0.0, 0.0, -0.0005765904285988091, -0.0062559241706160685, 0.0034397095356392834, -0.010414657666345174, 0.011102454226723808, 0.0011538461538461334, 0.004552352048558341, -0.014595808383233532, 0.0, -0.00057001710051294, 0.0, 0.008633921719109792, 0.0040548368410890075, 0.028252931001345474, -0.011769101438445716, 0.0, 0.009559512652296182, 0.0011139992573338086]\n",
      "33\n",
      "0.12290210825278859\n"
     ]
    }
   ],
   "source": [
    "#env_val = gym.make('stocks-v0', df=df, frame_bound=(400,420), window_size=5)\n",
    "#mid = int((val_start_idx+1+val_end_idx+1)/2)\n",
    "#env_val2 = MyCustomEnv(df=df, frame_bound=(val_start_idx,val_start_idx+30), window_size=12)\n",
    "env_val2 = StocksEnv2(df=df, frame_bound=(val_start_idx,val_end_idx), window_size=30)\n",
    "env_val2 = StocksEnv2(df=df, frame_bound=(test_start_idx,test_end_idx), window_size=30)\n",
    "obs_val = env_val2.reset()\n",
    "\n",
    "#obs_val = env_val2.reset()\n",
    "sharpe ,sortino = DRL_validation(df=df,model=model,test_env=env_val2, test_obs=obs_val)\n",
    "\n",
    "print(sharpe)\n",
    "#print(sortino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.cla()\n",
    "env_val2.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = model.get_env()\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, warn=False)\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def sharpeRatio(Ret):\n",
    "\tT = len(Ret)\n",
    "\tmean_ret = float(sum(Ret))/T\n",
    "\tmean_sq_ret = float(sum(Ret**2))/T\n",
    "\tif (mean_ret == 0.0) & (mean_sq_ret == 0.0):\n",
    "\t\treturn 0\n",
    "\tsharpe = mean_ret/sqrt(mean_sq_ret - mean_ret*mean_ret)\n",
    "\treturn sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "stock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
